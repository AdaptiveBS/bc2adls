{"cells":[{"cell_type":"markdown","id":"ae53e9bf-8787-4d07-b709-d896fd16cc5f","metadata":{"editable":false,"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"source":["## Business Central merge data notebook\n","In this part the files in the delta folder will be merge with the Lakehouse table.\n","- It iterates first on the folders to append to the existing table.\n","- After that is will remove all duplicates by sorting the table. \n","- At last it will remove all deleted records inside the table that are deleted in Business Central\n","\n","Please change the parameters in the first part."]},{"cell_type":"code","execution_count":11,"id":"34dc5721-e317-4dc0-88ef-2c6bafb494da","metadata":{"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.6812441Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:06.8530455Z\",\"execution_finish_time\":\"2023-08-15T09:15:07.1828235Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-08-15T09:15:07.1828235Z","execution_start_time":"2023-08-15T09:15:06.8530455Z","livy_statement_state":"available","parent_msg_id":"c1dad990-faf6-4bca-a50b-acb0e16459c7","queued_time":"2023-08-15T09:15:05.6812441Z","session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":13},"text/plain":["StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 13, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["%%pyspark\n","# settings\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\",\"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizewrite.enabled\",\"true\")\n","spark.conf.set(\"spark.sql.parquet.filterPushdown\", \"true\")\n","spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n","spark.conf.set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n","spark.conf.set(\"spark.sql.delta.commitProtocol.enabled\", \"true\")\n","\n","# parameters\n","folder_path_spark = 'Files/deltas/' # this is mostly the default\n","folder_path = '/lakehouse/default/Files/deltas/' # this is mostly the default\n","workspace = 'fabricTest' #can also be a GUID\n","Lakehouse = 'businessCentral'; #can also be a GUID\n","Remove_delta = True;"]},{"cell_type":"code","execution_count":12,"id":"0594c099-6512-4777-82e2-9a3a058512fe","metadata":{"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.7249665Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:07.7601315Z\",\"execution_finish_time\":\"2023-08-15T09:15:18.128035Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false,"editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-08-15T09:15:18.128035Z","execution_start_time":"2023-08-15T09:15:07.7601315Z","livy_statement_state":"available","parent_msg_id":"c48b6efe-47b4-4942-81e6-06224cb1472a","queued_time":"2023-08-15T09:15:05.7249665Z","session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-08-15T09:15:17.480GMT","dataRead":4997,"dataWritten":0,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":85,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[150,151,149],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:17.442GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:17.423GMT","dataRead":3068,"dataWritten":4997,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":84,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[147,148],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:16.889GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:16.761GMT","dataRead":7385,"dataWritten":3068,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":83,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[146],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:16.691GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:16.060GMT","dataRead":7366,"dataWritten":21728,"description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":82,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":9,"rowCount":96,"stageIds":[144,145],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:15.725GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:15.688GMT","dataRead":0,"dataWritten":7366,"description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":81,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":48,"stageIds":[143],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:15.663GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:13.446GMT","dataRead":6197,"dataWritten":0,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":80,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[140,141,142],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:13.413GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:13.395GMT","dataRead":4281,"dataWritten":6197,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":79,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[139,138],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:12.637GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:12.482GMT","dataRead":13078,"dataWritten":4281,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":78,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[137],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:12.418GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:11.800GMT","dataRead":5861,"dataWritten":52033,"description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":77,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":7,"rowCount":12,"stageIds":[135,136],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:11.413GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:11.366GMT","dataRead":0,"dataWritten":5861,"description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":76,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":6,"numCompletedStages":1,"numCompletedTasks":6,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":6,"rowCount":6,"stageIds":[134],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:11.333GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":10,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":14},"text/plain":["StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 14, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n","/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"]}],"source":["%%pyspark\n","import json\n","import os\n","from pyspark.sql.types import *\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.functions import col\n","\n","for entry in os.scandir(folder_path):\n"," if entry.is_dir():\n","    table_name = entry.name.replace(\"-\",\"\")\n","    \n","    for filename in os.listdir(folder_path + entry.name):\n","        # if there is a reset in the file then drop the whole table\n","        if \"-reset\" in filename:\n","            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n","\n","        df_spark = spark.read.format(\"csv\").option(\"header\",\"true\").load(folder_path_spark + entry.name +\"/\"+ filename)\n","        \n","        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n","        schema = json.load(f)\n","        # Parse the schema to get column names and data types\n","        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n","        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n","        for col_name, col_type in zip(column_names, column_types):\n","            if col_type == \"String\":\n","                col_type = \"string\"\n","            if col_type == \"Guid\":\n","                col_type = \"string\"\n","            if col_type == \"Code\":\n","                col_type = \"object\"\n","            if col_type == \"Option\":\n","                col_type = \"string\"\n","            if col_type == \"Date\":\n","                col_type = \"date\"\n","            if col_type == \"DateTime\":\n","                col_type = \"date\"\n","            if col_type == \"Duration\":\n","                col_type = \"timedelta\"\n","            if col_type == \"Decimal\":\n","                col_type = \"float\"\n","            if col_type == \"Boolean\":\n","                col_type = \"boolean\"\n","            if col_type == \"Integer\":\n","                col_type = \"int\"\n","            if col_type == \"Int64\":\n","                col_type = \"int\"\n","            if col_type == \"Int32\":\n","                col_type = \"int\"\n","\n","            df_spark = df_spark.withColumn(col_name, df_spark[col_name].cast(col_type))\n","\n","        df_spark.write.mode(\"append\").format(\"delta\").save(\"Tables/\" + table_name)\n","\n","        #delete the file\n","        if Remove_delta:\n","          os.remove(folder_path + entry.name +\"/\" + filename)"]},{"cell_type":"code","execution_count":13,"id":"35a74c88-ad69-46da-93b4-3d322e75fd9c","metadata":{"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.7722948Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:18.7196318Z\",\"execution_finish_time\":\"2023-08-15T09:15:25.517399Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false,"editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-08-15T09:15:25.517399Z","execution_start_time":"2023-08-15T09:15:18.7196318Z","livy_statement_state":"available","parent_msg_id":"32400866-adde-4217-b620-cb6d4ef506bb","queued_time":"2023-08-15T09:15:05.7722948Z","session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-08-15T09:15:24.266GMT","dataRead":5002,"dataWritten":0,"description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"15","jobId":101,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[179,177,178],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:24.240GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:24.224GMT","dataRead":5551,"dataWritten":5002,"description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"15","jobId":100,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[176,175],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:23.804GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:23.681GMT","dataRead":11198,"dataWritten":5551,"description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"15","jobId":99,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[174],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:23.618GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:23.140GMT","dataRead":3311,"dataWritten":0,"description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"15","jobId":98,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[172,173],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:23.042GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:22.943GMT","dataRead":7626,"dataWritten":21739,"description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"15","jobId":97,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":9,"rowCount":96,"stageIds":[171,170],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:22.640GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:22.566GMT","dataRead":0,"dataWritten":7626,"description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"15","jobId":96,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":48,"stageIds":[169],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:22.544GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:22.337GMT","dataRead":16572,"dataWritten":0,"description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","jobGroup":"15","jobId":95,"killedTasksSummary":{},"name":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":48,"stageIds":[168],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:22.232GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:22.177GMT","dataRead":3311,"dataWritten":0,"description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Filtering files for query","displayName":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","jobGroup":"15","jobId":94,"killedTasksSummary":{},"name":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[166,167],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:22.055GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:21.692GMT","dataRead":6202,"dataWritten":0,"description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"15","jobId":93,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[165,163,164],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:21.664GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:21.646GMT","dataRead":6792,"dataWritten":6202,"description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"15","jobId":92,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[161,162],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:21.175GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:21.053GMT","dataRead":16616,"dataWritten":6792,"description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"15","jobId":91,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[160],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:20.975GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:20.471GMT","dataRead":4520,"dataWritten":0,"description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"15","jobId":90,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[158,159],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:20.364GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:20.253GMT","dataRead":5861,"dataWritten":52061,"description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"15","jobId":89,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":7,"rowCount":12,"stageIds":[157,156],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:19.861GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:19.818GMT","dataRead":0,"dataWritten":5861,"description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"15","jobId":88,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":6,"numCompletedStages":1,"numCompletedTasks":6,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":6,"rowCount":6,"stageIds":[155],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:19.776GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:19.388GMT","dataRead":36982,"dataWritten":0,"description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","jobGroup":"15","jobId":87,"killedTasksSummary":{},"name":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":6,"stageIds":[154],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:19.266GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:19.210GMT","dataRead":4520,"dataWritten":0,"description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Filtering files for query","displayName":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","jobGroup":"15","jobId":86,"killedTasksSummary":{},"name":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[153,152],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:19.100GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":16,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":15},"text/plain":["StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 15, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n","/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"]}],"source":["%%pyspark\n","from pyspark.sql.types import *\n","import json\n","\n","# Open the manifest.cdm.json to read all the tables\n","f = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\n","schema = json.load(f)\n","\n","table_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\n","for table_name, in zip(table_name):\n","    table_name = table_name.replace(\"-\",\"\")\n","\n","    # remove deletes  \n","    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;  \n","    df_spark = spark.sql(SQL_Query)  \n","    df_deletes = df_spark.filter(df_spark['SystemCreatedAt-2000000001'].isNull())  \n","      \n","    for row in df_deletes.collect():  \n","        df_spark = df_spark.filter(df_spark['systemId-2000000000'] != row['systemId-2000000000'])\n","  \n","    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name) "]},{"cell_type":"code","execution_count":14,"id":"b0d334a5-db87-4c9b-9717-fb7a0d42d670","metadata":{"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.8150442Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:26.1031921Z\",\"execution_finish_time\":\"2023-08-15T09:15:32.8464445Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-08-15T09:15:32.8464445Z","execution_start_time":"2023-08-15T09:15:26.1031921Z","livy_statement_state":"available","parent_msg_id":"661af7d5-3577-4148-9a86-998a9094879e","queued_time":"2023-08-15T09:15:05.8150442Z","session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-08-15T09:15:31.552GMT","dataRead":15011,"dataWritten":8034,"description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"16","jobId":115,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":20,"stageIds":[202],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:31.480GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:30.955GMT","dataRead":3512,"dataWritten":0,"description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"16","jobId":114,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":4,"stageIds":[201,200],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:30.810GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:30.691GMT","dataRead":7626,"dataWritten":21739,"description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"16","jobId":113,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":9,"rowCount":96,"stageIds":[198,199],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:30.358GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:30.321GMT","dataRead":0,"dataWritten":7626,"description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"16","jobId":112,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":48,"stageIds":[197],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:30.302GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:30.108GMT","dataRead":16583,"dataWritten":0,"description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","jobGroup":"16","jobId":111,"killedTasksSummary":{},"name":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":48,"stageIds":[196],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:30.006GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:29.962GMT","dataRead":3512,"dataWritten":0,"description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Filtering files for query","displayName":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","jobGroup":"16","jobId":110,"killedTasksSummary":{},"name":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":4,"stageIds":[194,195],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:29.784GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:28.996GMT","dataRead":6207,"dataWritten":0,"description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"16","jobId":109,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[191,192,193],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:28.969GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:28.947GMT","dataRead":9308,"dataWritten":6207,"description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"16","jobId":108,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":60,"stageIds":[190,189],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:28.489GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:28.364GMT","dataRead":20154,"dataWritten":9308,"description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"16","jobId":107,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":20,"stageIds":[188],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:28.290GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:27.792GMT","dataRead":4721,"dataWritten":0,"description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"16","jobId":106,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":4,"stageIds":[186,187],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:27.679GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:27.577GMT","dataRead":5861,"dataWritten":52061,"description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"16","jobId":105,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":7,"rowCount":12,"stageIds":[184,185],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:27.230GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:27.177GMT","dataRead":0,"dataWritten":5861,"description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"16","jobId":104,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":6,"numCompletedStages":1,"numCompletedTasks":6,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":6,"rowCount":6,"stageIds":[183],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:27.149GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:26.915GMT","dataRead":37010,"dataWritten":0,"description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","displayName":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","jobGroup":"16","jobId":103,"killedTasksSummary":{},"name":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":6,"stageIds":[182],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:26.746GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:26.693GMT","dataRead":4721,"dataWritten":0,"description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Filtering files for query","displayName":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","jobGroup":"16","jobId":102,"killedTasksSummary":{},"name":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":4,"stageIds":[180,181],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:26.578GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":14,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":16},"text/plain":["StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 16, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n","/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"]}],"source":["%%pyspark\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import desc\n","import json\n","\n","# Open the manifest.cdm.json to read all the tables\n","f = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\n","schema = json.load(f)\n","\n","table_names = [attr[\"entityName\"] for attr in schema[\"entities\"]]  \n","for table_name in table_names:  \n","    table_name = table_name.replace(\"-\",\"\")\n","\n","    # remove duplicates by filtering on systemID and systemModifiedAt fields\n","    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n","    df_spark = spark.sql(SQL_Query)\n","    df_spark = df_spark.orderBy('systemId-2000000000',desc('SystemModifiedAt-2000000003'))\n","    df_spark = df_spark.dropDuplicates(['systemId-2000000000'])\n","    \n","    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{"synapse_widget":{"state":{},"token":"a69b4b72-86b0-4373-b695-ef01cd53bbb1"},"trident":{"lakehouse":{"default_lakehouse":"9fbacb3e-d0df-43a4-814b-abe4cb623a81","known_lakehouses":"[{\"id\":\"9fbacb3e-d0df-43a4-814b-abe4cb623a81\"}]"}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"9fbacb3e-d0df-43a4-814b-abe4cb623a81","default_lakehouse_name":"businessCentral","default_lakehouse_workspace_id":"21a92229-a0fb-4256-86bd-4b847b8006ed","known_lakehouses":[{"id":"9fbacb3e-d0df-43a4-814b-abe4cb623a81"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
