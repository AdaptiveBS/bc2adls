{"cells":[{"cell_type":"markdown","id":"ae53e9bf-8787-4d07-b709-d896fd16cc5f","metadata":{"editable":false,"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"source":["## Business Central merge data notebook\n","In this part the files in the delta folder will be merge with the Lakehouse table.\n","- It iterates first on the folders to append to the existing table.\n","- After that is will remove all duplicates by sorting the table. \n","- At last it will remove all deleted records inside the table that are deleted in Business Central\n","\n","Please change the parameters in the first part."]},{"cell_type":"code","execution_count":11,"id":"34dc5721-e317-4dc0-88ef-2c6bafb494da","metadata":{"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.6812441Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:06.8530455Z\",\"execution_finish_time\":\"2023-08-15T09:15:07.1828235Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-08-15T09:15:07.1828235Z","execution_start_time":"2023-08-15T09:15:06.8530455Z","livy_statement_state":"available","parent_msg_id":"c1dad990-faf6-4bca-a50b-acb0e16459c7","queued_time":"2023-08-15T09:15:05.6812441Z","session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":13},"text/plain":["StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 13, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["%%pyspark\n","# settings\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\",\"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizewrite.enabled\",\"true\")\n","spark.conf.set(\"spark.sql.parquet.filterPushdown\", \"true\")\n","spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n","spark.conf.set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n","spark.conf.set(\"spark.sql.delta.commitProtocol.enabled\", \"true\")\n","\n","# parameters\n","folder_path_spark = 'Files/deltas/' # this is mostly the default\n","folder_path = '/lakehouse/default/Files/deltas/' # this is mostly the default\n","workspace = 'fabricTest' #can also be a GUID\n","Lakehouse = 'businessCentral'; #can also be a GUID\n","Remove_delta = True;"]},{"cell_type":"code","execution_count":12,"id":"0594c099-6512-4777-82e2-9a3a058512fe","metadata":{"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.7249665Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:07.7601315Z\",\"execution_finish_time\":\"2023-08-15T09:15:18.128035Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false,"editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-08-15T09:15:18.128035Z","execution_start_time":"2023-08-15T09:15:07.7601315Z","livy_statement_state":"available","parent_msg_id":"c48b6efe-47b4-4942-81e6-06224cb1472a","queued_time":"2023-08-15T09:15:05.7249665Z","session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-08-15T09:15:17.480GMT","dataRead":4997,"dataWritten":0,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":85,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[150,151,149],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:17.442GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:17.423GMT","dataRead":3068,"dataWritten":4997,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":84,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[147,148],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:16.889GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:16.761GMT","dataRead":7385,"dataWritten":3068,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":83,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[146],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:16.691GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:16.060GMT","dataRead":7366,"dataWritten":21728,"description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":82,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":9,"rowCount":96,"stageIds":[144,145],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:15.725GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:15.688GMT","dataRead":0,"dataWritten":7366,"description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":81,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":48,"stageIds":[143],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:15.663GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:13.446GMT","dataRead":6197,"dataWritten":0,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":80,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[140,141,142],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:13.413GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:13.395GMT","dataRead":4281,"dataWritten":6197,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":79,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[139,138],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:12.637GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:12.482GMT","dataRead":13078,"dataWritten":4281,"description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"14","jobId":78,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[137],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:12.418GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:11.800GMT","dataRead":5861,"dataWritten":52033,"description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":77,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":7,"rowCount":12,"stageIds":[135,136],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:11.413GMT","usageDescription":""},{"completionTime":"2023-08-15T09:15:11.366GMT","dataRead":0,"dataWritten":5861,"description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","displayName":"save at NativeMethodAccessorImpl.java:0","jobGroup":"14","jobId":76,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":6,"numCompletedStages":1,"numCompletedTasks":6,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":6,"rowCount":6,"stageIds":[134],"status":"SUCCEEDED","submissionTime":"2023-08-15T09:15:11.333GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":10,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":14},"text/plain":["StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 14, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n","/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"]}],"source":["%%pyspark\n","import json\n","import os\n","import glob\n","from pyspark.sql.types import *\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.functions import col\n","from pyspark.sql.functions import desc\n","\n","for entry in os.scandir(folder_path):\n"," if entry.is_dir():\n","    if glob.glob(folder_path + entry.name + '/*'):\n","        table_name = entry.name.replace(\"-\",\"\")\n","\n","        df_new = spark.read.format(\"csv\").option(\"header\",\"true\").load(folder_path_spark + entry.name +\"/*\")   \n","        \n","        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n","        schema = json.load(f)\n","        # Parse the schema to get column names and data types\n","        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n","        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n","        for col_name, col_type in zip(column_names, column_types):\n","            if col_type == \"String\":\n","                col_type = \"string\"\n","            if col_type == \"Guid\":\n","                col_type = \"string\"\n","            if col_type == \"Code\":\n","                col_type = \"object\"\n","            if col_type == \"Option\":\n","                col_type = \"string\"\n","            if col_type == \"Date\":\n","                col_type = \"date\"\n","            if col_type == \"DateTime\":\n","                col_type = \"date\"\n","            if col_type == \"Duration\":\n","                col_type = \"timedelta\"\n","            if col_type == \"Decimal\":\n","                col_type = \"float\"\n","            if col_type == \"Boolean\":\n","                col_type = \"boolean\"\n","            if col_type == \"Integer\":\n","                col_type = \"int\"\n","            if col_type == \"Int64\":\n","                col_type = \"int\"\n","            if col_type == \"Int32\":\n","                col_type = \"int\"\n","\n","            df_new = df_new.withColumn(col_name, df_new[col_name].cast(col_type))\n","\n","        #check if the table excists\n","        if table_name in [t.name for t in spark.catalog.listTables()]:  \n","            #read the old data into a new dataframe and union with the new dataframe\n","            SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;  \n","            df_old = spark.sql(SQL_Query)\n","            df_new = df_new.union(df_old)\n","\n","            #delete all old records\n","            df_deletes = df_new.filter(df_new['SystemCreatedAt-2000000001'].isNull())\n","            df_new = df_new.join(df_deletes, [\"systemId-2000000000\"], \"leftanti\")\n","\n","            # remove duplicates by filtering on systemID and systemModifiedAt fields\n","            df_new = df_new.orderBy('systemId-2000000000',desc('SystemModifiedAt-2000000003'))\n","            df_new = df_new.dropDuplicates(['systemId-2000000000'])\n","\n","            #overwrite the dataframe in the new table\n","            df_new.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name) \n","        else:  \n","            #table isn't there so just insert it\n","            df_new.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","\n","        #delete the files\n","        if Remove_delta:  \n","            files = glob.glob(folder_path + entry.name + \"/*\")  \n","            for f in files:\n","                os.remove(f)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{"synapse_widget":{"state":{},"token":"a69b4b72-86b0-4373-b695-ef01cd53bbb1"},"trident":{"lakehouse":{"default_lakehouse":"9fbacb3e-d0df-43a4-814b-abe4cb623a81","known_lakehouses":"[{\"id\":\"9fbacb3e-d0df-43a4-814b-abe4cb623a81\"}]"}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"9fbacb3e-d0df-43a4-814b-abe4cb623a81","default_lakehouse_name":"businessCentral","default_lakehouse_workspace_id":"21a92229-a0fb-4256-86bd-4b847b8006ed","known_lakehouses":[{"id":"9fbacb3e-d0df-43a4-814b-abe4cb623a81"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
