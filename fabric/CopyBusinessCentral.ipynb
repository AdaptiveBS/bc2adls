{"cells":[{"cell_type":"markdown","source":["## Business Central merge data notebook\n","In this part the files in the delta folder will be merge with the Lakehouse table.\n","- It iterates first on the folders to append to the existing table.\n","- After that is will remove all duplicates by sorting the table. \n","- At last it will remove all deleted records inside the table that are deleted in Business Central\n","\n","Please change the parameters in the first part."],"metadata":{"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":false},"id":"ae53e9bf-8787-4d07-b709-d896fd16cc5f"},{"cell_type":"code","source":["%%pyspark\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\",\"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizewrite.enabled\",\"true\")\n","\n","# parameters\n","folder_path = '/lakehouse/default/Files/deltas/' # this is mostly the default\n","workspace = 'fabricTest' #can also be a GUID\n","Lakehouse = 'businessCentral'; #can also be a GUID\n","Remove_delta = True;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-15T09:15:05.6812441Z","session_start_time":null,"execution_start_time":"2023-08-15T09:15:06.8530455Z","execution_finish_time":"2023-08-15T09:15:07.1828235Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c1dad990-faf6-4bca-a50b-acb0e16459c7"},"text/plain":"StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 13, Finished, Available)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python"},"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.6812441Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:06.8530455Z\",\"execution_finish_time\":\"2023-08-15T09:15:07.1828235Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","run_control":{"frozen":false},"editable":false},"id":"34dc5721-e317-4dc0-88ef-2c6bafb494da"},{"cell_type":"code","source":["%%pyspark\n","import json\n","import os\n","import pandas as pd\n","from pyspark.sql.types import *\n","from pyspark.sql.utils import AnalysisException\n","\n","for entry in os.scandir(folder_path):\n"," if entry.is_dir():\n","    table_name = entry.name.replace(\"-\",\"\")\n","    \n","    for filename in os.listdir(folder_path + entry.name):\n","        # if there is a reset in the file then drop the whole table\n","        if filename.find(\"-reset\"):\n","            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n","\n","        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n","        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n","        schema = json.load(f)\n","        # Parse the schema to get column names and data types\n","        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n","        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n","        for col_name, col_type in zip(column_names, column_types):\n","            if col_type == \"String\":\n","                col_type = \"str\"\n","            if col_type == \"Guid\":\n","                col_type = \"str\"\n","            if col_type == \"Code\":\n","                col_type = \"object\"\n","            if col_type == \"Option\":\n","                col_type = \"str\"\n","            if col_type == \"Date\":\n","                col_type = \"datetime64[ns]\"\n","            if col_type == \"DateTime\":\n","                col_type = \"datetime64[ns]\"\n","            if col_type == \"Duration\":\n","                col_type = \"timedelta[ns]\"\n","            if col_type == \"Decimal\":\n","                col_type = \"float64\"\n","            if col_type == \"Boolean\":\n","                col_type = \"bool\"\n","            if col_type == \"Integer\":\n","                col_type = \"int\"\n","            if col_type == \"Int64\":\n","                col_type = \"int\"\n","            df[col_name] = df[col_name].astype(col_type)\n","\n","        df_spark = spark.createDataFrame(df)\n","        df_spark.write.mode(\"append\").format(\"delta\").save(\"Tables/\" + table_name)\n","\n","        #delete the file\n","        if Remove_delta:\n","          os.remove(folder_path + entry.name +\"/\" + filename)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-15T09:15:05.7249665Z","session_start_time":null,"execution_start_time":"2023-08-15T09:15:07.7601315Z","execution_finish_time":"2023-08-15T09:15:18.128035Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":10,"UNKNOWN":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4997,"rowCount":50,"usageDescription":"","jobId":85,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","submissionTime":"2023-08-15T09:15:17.442GMT","completionTime":"2023-08-15T09:15:17.480GMT","stageIds":[150,151,149],"jobGroup":"14","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4997,"dataRead":3068,"rowCount":54,"usageDescription":"","jobId":84,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","submissionTime":"2023-08-15T09:15:16.889GMT","completionTime":"2023-08-15T09:15:17.423GMT","stageIds":[147,148],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":3068,"dataRead":7385,"rowCount":8,"usageDescription":"","jobId":83,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","submissionTime":"2023-08-15T09:15:16.691GMT","completionTime":"2023-08-15T09:15:16.761GMT","stageIds":[146],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":21728,"dataRead":7366,"rowCount":96,"usageDescription":"","jobId":82,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","submissionTime":"2023-08-15T09:15:15.725GMT","completionTime":"2023-08-15T09:15:16.060GMT","stageIds":[144,145],"jobGroup":"14","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":7366,"dataRead":0,"rowCount":48,"usageDescription":"","jobId":81,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","submissionTime":"2023-08-15T09:15:15.663GMT","completionTime":"2023-08-15T09:15:15.688GMT","stageIds":[143],"jobGroup":"14","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":6197,"rowCount":50,"usageDescription":"","jobId":80,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","submissionTime":"2023-08-15T09:15:13.413GMT","completionTime":"2023-08-15T09:15:13.446GMT","stageIds":[140,141,142],"jobGroup":"14","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":6197,"dataRead":4281,"rowCount":54,"usageDescription":"","jobId":79,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","submissionTime":"2023-08-15T09:15:12.637GMT","completionTime":"2023-08-15T09:15:13.395GMT","stageIds":[139,138],"jobGroup":"14","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4281,"dataRead":13078,"rowCount":8,"usageDescription":"","jobId":78,"name":"toString at String.java:2994","description":"Delta: Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...: Compute snapshot for version: 0","submissionTime":"2023-08-15T09:15:12.418GMT","completionTime":"2023-08-15T09:15:12.482GMT","stageIds":[137],"jobGroup":"14","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":52033,"dataRead":5861,"rowCount":12,"usageDescription":"","jobId":77,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","submissionTime":"2023-08-15T09:15:11.413GMT","completionTime":"2023-08-15T09:15:11.800GMT","stageIds":[135,136],"jobGroup":"14","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":5861,"dataRead":0,"rowCount":6,"usageDescription":"","jobId":76,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 14:\nimport json\nimport os\nimport pandas as pd\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\n\nfor entry in os.scandir(folder_path):\n if entry.is_dir():\n    table_name = entry.name.replace(\"-\",\"\")\n    \n    for filename in os.listdir(folder_path + entry.name):\n        # if there is a reset in the file then drop the whole table\n        if filename.find(\"-reset\"):\n            df = spark.sql(\"DROP TABLE IF EXISTS \" + Lakehouse +\".\" + table_name)\n\n        df = pd.read_csv(folder_path + entry.name +\"/\" + filename)\n        f = open(\"/lakehouse/default/Files/\"+ entry.name +\".cdm.json\")\n        schema = json.load(f)\n        # Parse the schema to get column names and data types\n        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n        for col_name, col_type in zip(column_names, column_types):\n            if col_type == \"String\":\n                col_t...","submissionTime":"2023-08-15T09:15:11.333GMT","completionTime":"2023-08-15T09:15:11.366GMT","stageIds":[134],"jobGroup":"14","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c48b6efe-47b4-4942-81e6-06224cb1472a"},"text/plain":"StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 14, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"]}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.7249665Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:07.7601315Z\",\"execution_finish_time\":\"2023-08-15T09:15:18.128035Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false,"microsoft":{"language":"python"},"run_control":{"frozen":false},"editable":false},"id":"0594c099-6512-4777-82e2-9a3a058512fe"},{"cell_type":"code","source":["%%pyspark\n","from pyspark.sql.types import *\n","import pandas as pd\n","import json\n","\n","# Open the manifest.cdm.json to read all the tables\n","f = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\n","schema = json.load(f)\n","\n","table_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\n","for table_name, in zip(table_name):\n","    table_name = table_name.replace(\"-\",\"\")\n","\n","    # remove duplicates by filtering on systemID and systemModifiedAt fields\n","    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n","    df_spark = spark.sql(SQL_Query)\n","    df_panda = df_spark.toPandas()\n","    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n","    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n","    \n","    df_spark = spark.createDataFrame(df_panda)    \n","    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","statement_id":15,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-15T09:15:05.7722948Z","session_start_time":null,"execution_start_time":"2023-08-15T09:15:18.7196318Z","execution_finish_time":"2023-08-15T09:15:25.517399Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":16,"UNKNOWN":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":5002,"rowCount":50,"usageDescription":"","jobId":101,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","submissionTime":"2023-08-15T09:15:24.240GMT","completionTime":"2023-08-15T09:15:24.266GMT","stageIds":[179,177,178],"jobGroup":"15","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":5002,"dataRead":5551,"rowCount":57,"usageDescription":"","jobId":100,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","submissionTime":"2023-08-15T09:15:23.804GMT","completionTime":"2023-08-15T09:15:24.224GMT","stageIds":[176,175],"jobGroup":"15","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":5551,"dataRead":11198,"rowCount":14,"usageDescription":"","jobId":99,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","submissionTime":"2023-08-15T09:15:23.618GMT","completionTime":"2023-08-15T09:15:23.681GMT","stageIds":[174],"jobGroup":"15","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":3311,"rowCount":3,"usageDescription":"","jobId":98,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:23.042GMT","completionTime":"2023-08-15T09:15:23.140GMT","stageIds":[172,173],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":21739,"dataRead":7626,"rowCount":96,"usageDescription":"","jobId":97,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:22.640GMT","completionTime":"2023-08-15T09:15:22.943GMT","stageIds":[171,170],"jobGroup":"15","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":7626,"dataRead":0,"rowCount":48,"usageDescription":"","jobId":96,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:22.544GMT","completionTime":"2023-08-15T09:15:22.566GMT","stageIds":[169],"jobGroup":"15","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","dataWritten":0,"dataRead":16572,"rowCount":48,"usageDescription":"","jobId":95,"name":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:22.232GMT","completionTime":"2023-08-15T09:15:22.337GMT","stageIds":[168],"jobGroup":"15","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","dataWritten":0,"dataRead":3311,"rowCount":3,"usageDescription":"","jobId":94,"name":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Filtering files for query","submissionTime":"2023-08-15T09:15:22.055GMT","completionTime":"2023-08-15T09:15:22.177GMT","stageIds":[166,167],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":6202,"rowCount":50,"usageDescription":"","jobId":93,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","submissionTime":"2023-08-15T09:15:21.664GMT","completionTime":"2023-08-15T09:15:21.692GMT","stageIds":[165,163,164],"jobGroup":"15","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":6202,"dataRead":6792,"rowCount":57,"usageDescription":"","jobId":92,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","submissionTime":"2023-08-15T09:15:21.175GMT","completionTime":"2023-08-15T09:15:21.646GMT","stageIds":[161,162],"jobGroup":"15","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":6792,"dataRead":16616,"rowCount":14,"usageDescription":"","jobId":91,"name":"toString at String.java:2994","description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","submissionTime":"2023-08-15T09:15:20.975GMT","completionTime":"2023-08-15T09:15:21.053GMT","stageIds":[160],"jobGroup":"15","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":4520,"rowCount":3,"usageDescription":"","jobId":90,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:20.364GMT","completionTime":"2023-08-15T09:15:20.471GMT","stageIds":[158,159],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":52061,"dataRead":5861,"rowCount":12,"usageDescription":"","jobId":89,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:19.861GMT","completionTime":"2023-08-15T09:15:20.253GMT","stageIds":[157,156],"jobGroup":"15","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":5861,"dataRead":0,"rowCount":6,"usageDescription":"","jobId":88,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:19.776GMT","completionTime":"2023-08-15T09:15:19.818GMT","stageIds":[155],"jobGroup":"15","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","dataWritten":0,"dataRead":36982,"rowCount":6,"usageDescription":"","jobId":87,"name":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","description":"Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:19.266GMT","completionTime":"2023-08-15T09:15:19.388GMT","stageIds":[154],"jobGroup":"15","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","dataWritten":0,"dataRead":4520,"rowCount":3,"usageDescription":"","jobId":86,"name":"toPandas at /tmp/ipykernel_7892/2520867349.py:16","description":"Delta: Job group for statement 15:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove duplicates by filtering on systemID and systemModifiedAt fields\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_panda = df_panda.sort_values(['systemId-2000000000','SystemModifiedAt-2000000003'], ascending=[True, False])\n    df_panda = df_panda.drop_duplicates(subset=['systemId-2000000000'], keep='first')    \n    \n    df_spark = spark.createDataFrame(df_panda)    \n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Filtering files for query","submissionTime":"2023-08-15T09:15:19.100GMT","completionTime":"2023-08-15T09:15:19.210GMT","stageIds":[153,152],"jobGroup":"15","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"32400866-adde-4217-b620-cb6d4ef506bb"},"text/plain":"StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 15, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"]}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python"},"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.7722948Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:18.7196318Z\",\"execution_finish_time\":\"2023-08-15T09:15:25.517399Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false,"run_control":{"frozen":false},"editable":false},"id":"35a74c88-ad69-46da-93b4-3d322e75fd9c"},{"cell_type":"code","source":["%%pyspark\n","from pyspark.sql.types import *\n","import pandas as pd\n","import json\n","\n","# Open the manifest.cdm.json to read all the tables\n","f = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\n","schema = json.load(f)\n","\n","table_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\n","for table_name, in zip(table_name):\n","    table_name = table_name.replace(\"-\",\"\")\n","\n","    # remove deletes\n","    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n","    df_spark = spark.sql(SQL_Query)\n","    df_panda = df_spark.toPandas()\n","    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n","    for index, row in df_deletes.iterrows(): \n","        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n","\n","    df_spark = spark.createDataFrame(df_panda)\n","    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"daf5b681-1577-45ce-aec5-3696d3564aa0","statement_id":16,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-15T09:15:05.8150442Z","session_start_time":null,"execution_start_time":"2023-08-15T09:15:26.1031921Z","execution_finish_time":"2023-08-15T09:15:32.8464445Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":14,"UNKNOWN":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":8034,"dataRead":15011,"rowCount":20,"usageDescription":"","jobId":115,"name":"toString at String.java:2994","description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 2","submissionTime":"2023-08-15T09:15:31.480GMT","completionTime":"2023-08-15T09:15:31.552GMT","stageIds":[202],"jobGroup":"16","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":3512,"rowCount":4,"usageDescription":"","jobId":114,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:30.810GMT","completionTime":"2023-08-15T09:15:30.955GMT","stageIds":[201,200],"jobGroup":"16","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":21739,"dataRead":7626,"rowCount":96,"usageDescription":"","jobId":113,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:30.358GMT","completionTime":"2023-08-15T09:15:30.691GMT","stageIds":[198,199],"jobGroup":"16","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":7626,"dataRead":0,"rowCount":48,"usageDescription":"","jobId":112,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:30.302GMT","completionTime":"2023-08-15T09:15:30.321GMT","stageIds":[197],"jobGroup":"16","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","dataWritten":0,"dataRead":16583,"rowCount":48,"usageDescription":"","jobId":111,"name":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:30.006GMT","completionTime":"2023-08-15T09:15:30.108GMT","stageIds":[196],"jobGroup":"16","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","dataWritten":0,"dataRead":3512,"rowCount":4,"usageDescription":"","jobId":110,"name":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Filtering files for query","submissionTime":"2023-08-15T09:15:29.784GMT","completionTime":"2023-08-15T09:15:29.962GMT","stageIds":[194,195],"jobGroup":"16","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":6207,"rowCount":50,"usageDescription":"","jobId":109,"name":"toString at String.java:2994","description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 2","submissionTime":"2023-08-15T09:15:28.969GMT","completionTime":"2023-08-15T09:15:28.996GMT","stageIds":[191,192,193],"jobGroup":"16","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":53,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":6207,"dataRead":9308,"rowCount":60,"usageDescription":"","jobId":108,"name":"toString at String.java:2994","description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 2","submissionTime":"2023-08-15T09:15:28.489GMT","completionTime":"2023-08-15T09:15:28.947GMT","stageIds":[190,189],"jobGroup":"16","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":9308,"dataRead":20154,"rowCount":20,"usageDescription":"","jobId":107,"name":"toString at String.java:2994","description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Compute snapshot for version: 2","submissionTime":"2023-08-15T09:15:28.290GMT","completionTime":"2023-08-15T09:15:28.364GMT","stageIds":[188],"jobGroup":"16","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","dataWritten":0,"dataRead":4721,"rowCount":4,"usageDescription":"","jobId":106,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:27.679GMT","completionTime":"2023-08-15T09:15:27.792GMT","stageIds":[186,187],"jobGroup":"16","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":52061,"dataRead":5861,"rowCount":12,"usageDescription":"","jobId":105,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:27.230GMT","completionTime":"2023-08-15T09:15:27.577GMT","stageIds":[184,185],"jobGroup":"16","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":5861,"dataRead":0,"rowCount":6,"usageDescription":"","jobId":104,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:27.149GMT","completionTime":"2023-08-15T09:15:27.177GMT","stageIds":[183],"jobGroup":"16","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","dataWritten":0,"dataRead":37010,"rowCount":6,"usageDescription":"","jobId":103,"name":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","description":"Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)","submissionTime":"2023-08-15T09:15:26.746GMT","completionTime":"2023-08-15T09:15:26.915GMT","stageIds":[182],"jobGroup":"16","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","dataWritten":0,"dataRead":4721,"rowCount":4,"usageDescription":"","jobId":102,"name":"toPandas at /tmp/ipykernel_7892/3697614253.py:16","description":"Delta: Job group for statement 16:\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport json\n\n# Open the manifest.cdm.json to read all the tables\nf = open(\"/lakehouse/default/Files/deltas.manifest.cdm.json\")\nschema = json.load(f)\n\ntable_name = [attr[\"entityName\"] for attr in schema[\"entities\"]]\nfor table_name, in zip(table_name):\n    table_name = table_name.replace(\"-\",\"\")\n\n    # remove deletes\n    SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;\n    df_spark = spark.sql(SQL_Query)\n    df_panda = df_spark.toPandas()\n    df_deletes = df_panda[df_panda['SystemCreatedAt-2000000001'].isnull()]\n    for index, row in df_deletes.iterrows(): \n        df_panda.drop(df_panda[df_panda['systemId-2000000000'] == row['systemId-2000000000']].index, inplace=True)\n\n    df_spark = spark.createDataFrame(df_panda)\n    df_spark.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name): Filtering files for query","submissionTime":"2023-08-15T09:15:26.578GMT","completionTime":"2023-08-15T09:15:26.693GMT","stageIds":[180,181],"jobGroup":"16","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"661af7d5-3577-4148-9a86-998a9094879e"},"text/plain":"StatementMeta(, daf5b681-1577-45ce-aec5-3696d3564aa0, 16, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"]}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python"},"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.8150442Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:26.1031921Z\",\"execution_finish_time\":\"2023-08-15T09:15:32.8464445Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","run_control":{"frozen":false},"editable":false},"id":"b0d334a5-db87-4c9b-9717-fb7a0d42d670"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"host":{"trident":{"lakehouse":{"known_lakehouses":"[{\"id\":\"9fbacb3e-d0df-43a4-814b-abe4cb623a81\"}]","default_lakehouse":"9fbacb3e-d0df-43a4-814b-abe4cb623a81"}},"synapse_widget":{"token":"a69b4b72-86b0-4373-b695-ef01cd53bbb1","state":{}}}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"9fbacb3e-d0df-43a4-814b-abe4cb623a81","known_lakehouses":[{"id":"9fbacb3e-d0df-43a4-814b-abe4cb623a81"}],"default_lakehouse_name":"businessCentral","default_lakehouse_workspace_id":"21a92229-a0fb-4256-86bd-4b847b8006ed"}}},"nbformat":4,"nbformat_minor":5}